{"nbformat": 4, "cells": [{"source": ["\n<h4 id=\"In-this-example,-we-illustrate-various-unsupervised-learning-techniques-(Clustering,-PCA,-SVD)-using-an-example-term-document-matrix-as-the-data.\">In this example, we illustrate various unsupervised learning techniques (Clustering, PCA, SVD) using an example term-document matrix as the data.<a class=\"anchor-link\" href=\"https://facweb.cs.depaul.edu/mobasher/classes/CSC478/Notes/Docs-Terms-Clustering-PCA-SVD.html#In-this-example,-we-illustrate-various-unsupervised-learning-techniques-(Clustering,-PCA,-SVD)-using-an-example-term-document-matrix-as-the-data.\">\u00b6</a></h4>\n"], "cell_type": "markdown", "metadata": {}}, {"execution_count": null, "outputs": [], "cell_type": "code", "source": ["\nimport numpy as np\nimport pylab as pl\nimport pandas as pd\nfrom sklearn.cluster import KMeans \n\n"], "metadata": {}}, {"execution_count": null, "outputs": [], "cell_type": "code", "source": ["\ncd D:\\Documents\\Class\\CSC478\\Data\n\n"], "metadata": {}}, {"execution_count": null, "outputs": [], "cell_type": "code", "source": ["\nData = pd.read_csv('term-doc-mat.csv', header=None)\nTD = Data.ix[:,1:]\nTD\n\n"], "metadata": {}}, {"execution_count": null, "outputs": [], "cell_type": "code", "source": ["\nterms = Data.ix[:,0]\nterms\n\n"], "metadata": {}}, {"source": ["\n<h4 id=\"First,-we-want-to-do-some-document-clustering.-Since-the-data-is-in-term-document-format,-we-need-to-obtain-the-transpose-of-the-TD-matrix.\">First, we want to do some document clustering. Since the data is in term-document format, we need to obtain the transpose of the TD matrix.<a class=\"anchor-link\" href=\"https://facweb.cs.depaul.edu/mobasher/classes/CSC478/Notes/Docs-Terms-Clustering-PCA-SVD.html#First,-we-want-to-do-some-document-clustering.-Since-the-data-is-in-term-document-format,-we-need-to-obtain-the-transpose-of-the-TD-matrix.\">\u00b6</a></h4>\n"], "cell_type": "markdown", "metadata": {}}, {"execution_count": null, "outputs": [], "cell_type": "code", "source": ["\nDT = TD.T\n\n"], "metadata": {}}, {"source": ["\n<h4 id=\"Now-we-have-a-document-term-matrix:\">Now we have a document-term matrix:<a class=\"anchor-link\" href=\"https://facweb.cs.depaul.edu/mobasher/classes/CSC478/Notes/Docs-Terms-Clustering-PCA-SVD.html#Now-we-have-a-document-term-matrix:\">\u00b6</a></h4>\n"], "cell_type": "markdown", "metadata": {}}, {"execution_count": null, "outputs": [], "cell_type": "code", "source": ["\nDT\n\n"], "metadata": {}}, {"execution_count": null, "outputs": [], "cell_type": "code", "source": ["\nnumTerms=len(terms)\nnumTerms\n\n"], "metadata": {}}, {"source": ["\n<h4 id=\"Next,-we-will-transform-the-data-to-TFxIDF-weights:\">Next, we will transform the data to TFxIDF weights:<a class=\"anchor-link\" href=\"https://facweb.cs.depaul.edu/mobasher/classes/CSC478/Notes/Docs-Terms-Clustering-PCA-SVD.html#Next,-we-will-transform-the-data-to-TFxIDF-weights:\">\u00b6</a></h4>\n"], "cell_type": "markdown", "metadata": {}}, {"execution_count": null, "outputs": [], "cell_type": "code", "source": ["\n# Find doucment frequencies for each term\nDF = np.array([(DT!=0).sum(0)])\nprint DF\n\n"], "metadata": {}}, {"execution_count": null, "outputs": [], "cell_type": "code", "source": ["\nNDocs = len(DT[0])\nprint NDocs\n\n"], "metadata": {}}, {"execution_count": null, "outputs": [], "cell_type": "code", "source": ["\n# Create a matrix with all entries = NDocs\nNMatrix=np.ones(np.shape(DT), dtype=float)*NDocs\n\n"], "metadata": {}}, {"execution_count": null, "outputs": [], "cell_type": "code", "source": ["\n# Convert each entry into IDF values\n# Note that IDF is only a function of the term, so all rows will be identical.\nDivM = np.divide(NMatrix, DF)\nIDF = np.log2(DivM)\n\n"], "metadata": {}}, {"execution_count": null, "outputs": [], "cell_type": "code", "source": ["\nnp.set_printoptions(precision=2,suppress=True)\nprint IDF[0:2,]\n\n"], "metadata": {}}, {"execution_count": null, "outputs": [], "cell_type": "code", "source": ["\n# Finally compute the TFxIDF values for each document-term entry\nDT_tfidf = DT * IDF\n\n"], "metadata": {}}, {"execution_count": null, "outputs": [], "cell_type": "code", "source": ["\nDT_tfidf\n\n"], "metadata": {}}, {"source": ["\n<h4 id=\"Now-we-are-ready-for-clustering\">Now we are ready for clustering<a class=\"anchor-link\" href=\"https://facweb.cs.depaul.edu/mobasher/classes/CSC478/Notes/Docs-Terms-Clustering-PCA-SVD.html#Now-we-are-ready-for-clustering\">\u00b6</a></h4>\n"], "cell_type": "markdown", "metadata": {}}, {"execution_count": null, "outputs": [], "cell_type": "code", "source": ["\nimport kMeans\n\n"], "metadata": {}}, {"execution_count": null, "outputs": [], "cell_type": "code", "source": ["\nreload(kMeans)\n\n"], "metadata": {}}, {"execution_count": null, "outputs": [], "cell_type": "code", "source": ["\nDT_tfidf = np.array(DT_tfidf)\ncentroids_tfidf, clusters_tfidf = kMeans.kMeans(DT_tfidf, 3, kMeans.distCosine, kMeans.randCent)\n\n"], "metadata": {}}, {"source": ["\n<h4 id=\"Let's-take-a-look-at-the-cluster-centroids\">Let's take a look at the cluster centroids<a class=\"anchor-link\" href=\"https://facweb.cs.depaul.edu/mobasher/classes/CSC478/Notes/Docs-Terms-Clustering-PCA-SVD.html#Let's-take-a-look-at-the-cluster-centroids\">\u00b6</a></h4>\n"], "cell_type": "markdown", "metadata": {}}, {"execution_count": null, "outputs": [], "cell_type": "code", "source": ["\nprint \"\\t\\tCluster0\\tCluster1\\tCluster2\"\nfor i in range(len(terms)):\n    print \"%10s\\t%.4f\\t\\t%.4f\\t\\t%.4f\" %(terms[i],centroids_tfidf[0][i],centroids_tfidf[1][i],centroids_tfidf[2][i])\n\n"], "metadata": {}}, {"source": ["\n<h4 id=\"Because-the-centroids-are-based-on-TFxIDF-weights,-they-are-not-as-descriptive-as-raw-term-frequencies-or-binary-occurrence-data.-Let's-redo-the-clustering-with-the-original-raw-term-frequencies.\">Because the centroids are based on TFxIDF weights, they are not as descriptive as raw term frequencies or binary occurrence data. Let's redo the clustering with the original raw term frequencies.<a class=\"anchor-link\" href=\"https://facweb.cs.depaul.edu/mobasher/classes/CSC478/Notes/Docs-Terms-Clustering-PCA-SVD.html#Because-the-centroids-are-based-on-TFxIDF-weights,-they-are-not-as-descriptive-as-raw-term-frequencies-or-binary-occurrence-data.-Let's-redo-the-clustering-with-the-original-raw-term-frequencies.\">\u00b6</a></h4>\n"], "cell_type": "markdown", "metadata": {}}, {"execution_count": null, "outputs": [], "cell_type": "code", "source": ["\nDT = np.array(DT)\ncentroids, clusters = kMeans.kMeans(DT, 3, kMeans.distCosine, kMeans.randCent)\n\n"], "metadata": {}}, {"execution_count": null, "outputs": [], "cell_type": "code", "source": ["\nprint \"\\t\\tCluster0\\tCluster1\\tCluster2\"\nfor i in range(len(terms)):\n    print \"%10s\\t%.4f\\t\\t%.4f\\t\\t%.4f\" %(terms[i],centroids[0][i],centroids[1][i],centroids[2][i])\n\n"], "metadata": {}}, {"execution_count": null, "outputs": [], "cell_type": "code", "source": ["\nprint clusters\n\n"], "metadata": {}}, {"execution_count": null, "outputs": [], "cell_type": "code", "source": ["\nprint centroids\n\n"], "metadata": {}}, {"source": ["\n<h4 id=\"Next,-let's-use-principal-component-analysis-to-reduce-the-dimensionality-of-the-data:\">Next, let's use principal component analysis to reduce the dimensionality of the data:<a class=\"anchor-link\" href=\"https://facweb.cs.depaul.edu/mobasher/classes/CSC478/Notes/Docs-Terms-Clustering-PCA-SVD.html#Next,-let's-use-principal-component-analysis-to-reduce-the-dimensionality-of-the-data:\">\u00b6</a></h4>\n"], "cell_type": "markdown", "metadata": {}}, {"execution_count": null, "outputs": [], "cell_type": "code", "source": ["\nfrom sklearn import decomposition\n\n"], "metadata": {}}, {"source": ["\n<h4 id=\"We'll-perform-PCA-to-obtain-the-top-5-components-and-then-transform-the-DT-matrix-into-the-lower-dimensional-space-of-5-components:\">We'll perform PCA to obtain the top 5 components and then transform the DT matrix into the lower dimensional space of 5 components:<a class=\"anchor-link\" href=\"https://facweb.cs.depaul.edu/mobasher/classes/CSC478/Notes/Docs-Terms-Clustering-PCA-SVD.html#We'll-perform-PCA-to-obtain-the-top-5-components-and-then-transform-the-DT-matrix-into-the-lower-dimensional-space-of-5-components:\">\u00b6</a></h4>\n"], "cell_type": "markdown", "metadata": {}}, {"execution_count": null, "outputs": [], "cell_type": "code", "source": ["\npca = decomposition.PCA(n_components=5)\nDTtrans = pca.fit(DT).transform(DT)\n\n"], "metadata": {}}, {"execution_count": null, "outputs": [], "cell_type": "code", "source": ["\nnp.set_printoptions(precision=2,suppress=True)\nprint DTtrans\n\n"], "metadata": {}}, {"execution_count": null, "outputs": [], "cell_type": "code", "source": ["\nprint(pca.explained_variance_ratio_)\n\n"], "metadata": {}}, {"source": ["\n<h4 id=\"Looking-at-the-above,-it-can-be-obsereved-that-the-first-5-components-capture-(explain)-95%-of-the-variance-in-the-data.\">Looking at the above, it can be obsereved that the first 5 components capture (explain) 95% of the variance in the data.<a class=\"anchor-link\" href=\"https://facweb.cs.depaul.edu/mobasher/classes/CSC478/Notes/Docs-Terms-Clustering-PCA-SVD.html#Looking-at-the-above,-it-can-be-obsereved-that-the-first-5-components-capture-(explain)-95%-of-the-variance-in-the-data.\">\u00b6</a></h4>\n"], "cell_type": "markdown", "metadata": {}}, {"source": ["\n<h4 id=\"Now,-we-can-redo-the-clustering,-but-this-time-in-the-lower-dimensional-space:\">Now, we can redo the clustering, but this time in the lower dimensional space:<a class=\"anchor-link\" href=\"https://facweb.cs.depaul.edu/mobasher/classes/CSC478/Notes/Docs-Terms-Clustering-PCA-SVD.html#Now,-we-can-redo-the-clustering,-but-this-time-in-the-lower-dimensional-space:\">\u00b6</a></h4>\n"], "cell_type": "markdown", "metadata": {}}, {"execution_count": null, "outputs": [], "cell_type": "code", "source": ["\ncentroids_pca, clusters_pca = kMeans.kMeans(DTtrans, 3, kMeans.distCosine, kMeans.randCent)\n\n"], "metadata": {}}, {"execution_count": null, "outputs": [], "cell_type": "code", "source": ["\nprint clusters_pca\n\n"], "metadata": {}}, {"source": ["\n<h4 id=\"Next,-let's-actually-derive-the-principal-components-manaually-using-linear-algebra-rather-than-relying-on-the-PCA-package-from-sklearn:\">Next, let's actually derive the principal components manaually using linear algebra rather than relying on the PCA package from sklearn:<a class=\"anchor-link\" href=\"https://facweb.cs.depaul.edu/mobasher/classes/CSC478/Notes/Docs-Terms-Clustering-PCA-SVD.html#Next,-let's-actually-derive-the-principal-components-manaually-using-linear-algebra-rather-than-relying-on-the-PCA-package-from-sklearn:\">\u00b6</a></h4>\n"], "cell_type": "markdown", "metadata": {}}, {"source": ["\n<h4 id=\"First-step-is-to-obtain-the-covariance-matrix:\">First step is to obtain the covariance matrix:<a class=\"anchor-link\" href=\"https://facweb.cs.depaul.edu/mobasher/classes/CSC478/Notes/Docs-Terms-Clustering-PCA-SVD.html#First-step-is-to-obtain-the-covariance-matrix:\">\u00b6</a></h4>\n"], "cell_type": "markdown", "metadata": {}}, {"execution_count": null, "outputs": [], "cell_type": "code", "source": ["\nmeanVals = np.mean(DT, axis=0)\nmeanRemoved = DT - meanVals #remove mean\ncovMat = np.cov(meanRemoved, rowvar=0)\n\nnp.set_printoptions(precision=2,suppress=True,linewidth=100)\nprint covMat\n\n"], "metadata": {}}, {"execution_count": null, "outputs": [], "cell_type": "code", "source": ["\nimport numpy.linalg as la\neigVals,eigVects = la.eig(np.mat(covMat))\n\n"], "metadata": {}}, {"execution_count": null, "outputs": [], "cell_type": "code", "source": ["\nprint eigVals\n\n"], "metadata": {}}, {"execution_count": null, "outputs": [], "cell_type": "code", "source": ["\nprint eigVects\n\n"], "metadata": {}}, {"execution_count": null, "outputs": [], "cell_type": "code", "source": ["\neigValInd = np.argsort(eigVals)  #sort, sort goes smallest to largest\neigValInd = eigValInd[::-1]   #reverse\nsortedEigVals = eigVals[eigValInd]\nprint sortedEigVals\ntotal = sum(sortedEigVals)\nvarPercentage = sortedEigVals/total*100\nprint varPercentage\n\n"], "metadata": {}}, {"source": ["\n<h4 id=\"We-can-plot-the-principal-components-based-on-the-percentage-of-variance-they-capture:\">We can plot the principal components based on the percentage of variance they capture:<a class=\"anchor-link\" href=\"https://facweb.cs.depaul.edu/mobasher/classes/CSC478/Notes/Docs-Terms-Clustering-PCA-SVD.html#We-can-plot-the-principal-components-based-on-the-percentage-of-variance-they-capture:\">\u00b6</a></h4>\n"], "cell_type": "markdown", "metadata": {}}, {"execution_count": null, "outputs": [], "cell_type": "code", "source": ["\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nfig = plt.figure()\nax = fig.add_subplot(111)\nax.plot(range(1, 11), varPercentage[:10], marker='^')\nplt.xlabel('Principal Component Number')\nplt.ylabel('Percentage of Variance')\nplt.show()\n\n"], "metadata": {}}, {"execution_count": null, "outputs": [], "cell_type": "code", "source": ["\ntopNfeat = 5\ntopEigValInd = eigValInd[:topNfeat]  #cut off unwanted dimensions\nreducedEigVects = eigVects[:,topEigValInd]   #reorganize eig vects largest to smallest\nreducedDT = np.dot(meanRemoved, reducedEigVects)    #transform data into new dimensions\nprint reducedDT\n\n"], "metadata": {}}, {"source": ["\n<h4 id=\"Next,-let's-look-at-an-application-of-Singular-Value-Decomposition.-This-time,-we'll-foucs-on-the-term-document-matrix-in-order-to-find-themes-based-on-combinations-of-terms.\">Next, let's look at an application of Singular Value Decomposition. This time, we'll foucs on the term-document matrix in order to find themes based on combinations of terms.<a class=\"anchor-link\" href=\"https://facweb.cs.depaul.edu/mobasher/classes/CSC478/Notes/Docs-Terms-Clustering-PCA-SVD.html#Next,-let's-look-at-an-application-of-Singular-Value-Decomposition.-This-time,-we'll-foucs-on-the-term-document-matrix-in-order-to-find-themes-based-on-combinations-of-terms.\">\u00b6</a></h4>\n"], "cell_type": "markdown", "metadata": {}}, {"execution_count": null, "outputs": [], "cell_type": "code", "source": ["\nu, s, vt = la.svd(TD, full_matrices=False)\n\n"], "metadata": {}}, {"execution_count": null, "outputs": [], "cell_type": "code", "source": ["\nprint u\n\n"], "metadata": {}}, {"execution_count": null, "outputs": [], "cell_type": "code", "source": ["\nprint s\n\n"], "metadata": {}}, {"execution_count": null, "outputs": [], "cell_type": "code", "source": ["\nprint np.diag(s)\n\n"], "metadata": {}}, {"execution_count": null, "outputs": [], "cell_type": "code", "source": ["\noriginalTD = np.dot(u, np.dot(np.diag(s), vt))\nprint originalTD\n\n"], "metadata": {}}, {"execution_count": null, "outputs": [], "cell_type": "code", "source": ["\nnumDimensions = 3\nu_ld = u[:, :numDimensions]\nsigma = np.diag(s)[:numDimensions, :numDimensions]\nvt_ld = vt[:numDimensions, :]\nlowRankTD = np.dot(u_ld, np.dot(sigma, vt_ld))\n\n"], "metadata": {}}, {"execution_count": null, "outputs": [], "cell_type": "code", "source": ["\nnp.set_printoptions(precision=2,suppress=True,linewidth=120)\nprint lowRankTD\n\n"], "metadata": {}}, {"source": ["\n<h4 id=\"The-VT-matrix-can-be-viewed-as-the-new-representation-of-documents-in-the-lower-dimensional-space.\">The VT matrix can be viewed as the new representation of documents in the lower dimensional space.<a class=\"anchor-link\" href=\"https://facweb.cs.depaul.edu/mobasher/classes/CSC478/Notes/Docs-Terms-Clustering-PCA-SVD.html#The-VT-matrix-can-be-viewed-as-the-new-representation-of-documents-in-the-lower-dimensional-space.\">\u00b6</a></h4>\n"], "cell_type": "markdown", "metadata": {}}, {"execution_count": null, "outputs": [], "cell_type": "code", "source": ["\nprint vt_ld\n\n"], "metadata": {}}, {"source": ["\n<h4 id=\"In-information-retrieval,-a-query-is-compared-to-documents-using-vector-space-similarity-between-the-query-vector-and-document-vectors.-In-the-lower-dim.-space,-this-can-be-achieved-by-first-mapping-the-query-to-lower-dim.-space,-and-then-comparing-it-to-docs-in-the-lower-dim.-space.\">In information retrieval, a query is compared to documents using vector-space similarity between the query vector and document vectors. In the lower dim. space, this can be achieved by first mapping the query to lower dim. space, and then comparing it to docs in the lower dim. space.<a class=\"anchor-link\" href=\"https://facweb.cs.depaul.edu/mobasher/classes/CSC478/Notes/Docs-Terms-Clustering-PCA-SVD.html#In-information-retrieval,-a-query-is-compared-to-documents-using-vector-space-similarity-between-the-query-vector-and-document-vectors.-In-the-lower-dim.-space,-this-can-be-achieved-by-first-mapping-the-query-to-lower-dim.-space,-and-then-comparing-it-to-docs-in-the-lower-dim.-space.\">\u00b6</a></h4>\n"], "cell_type": "markdown", "metadata": {}}, {"execution_count": null, "outputs": [], "cell_type": "code", "source": ["\nqueryVector = np.array([0,0,1,5,4,0,6,0,0,2])\nlowDimQuery = np.dot(la.inv(sigma), np.dot(u_ld.T, queryVector))\nprint lowDimQuery\n\n"], "metadata": {}}, {"execution_count": null, "outputs": [], "cell_type": "code", "source": ["\n# Compute Cosine sim between the query and docs in the lower dimensional space\n\nqNorm = lowDimQuery / la.norm(lowDimQuery)\n\n"], "metadata": {}}, {"execution_count": null, "outputs": [], "cell_type": "code", "source": ["\ndocNorm = np.array([vt_ld[:,i]/la.norm(vt_ld[:,i]) for i in range(len(vt_ld[0]))])\t\t\nprint docNorm\n\n"], "metadata": {}}, {"execution_count": null, "outputs": [], "cell_type": "code", "source": ["\nsims = np.dot(qNorm, docNorm.T)\n# return indices of the docs in decending order of similarity to the query\nsimInds = sims.argsort()[::-1]\nfor i in simInds:\n    print \"Cosine similarity between Document %d and the query is: %.4f\" %(i,sims[i])\n\n"], "metadata": {}}, {"execution_count": null, "outputs": [], "cell_type": "code", "source": ["\ncentroids_svd, clusters_svd = kMeans.kMeans(vt_ld.T, 3, kMeans.distCosine, kMeans.randCent)\n\n"], "metadata": {}}, {"execution_count": null, "outputs": [], "cell_type": "code", "source": ["\nprint clusters_svd\n\n"], "metadata": {}}, {"execution_count": null, "outputs": [], "cell_type": "code", "source": ["\nprint \"\\t\\tCluster0\\tCluster1\\tCluster2\"\nfor i in range(numDimensions):\n    print \"Theme %d\\t\\t%.4f\\t\\t%.4f\\t\\t%.4f\" %(i,centroids_svd[0][i],centroids_svd[1][i],centroids_svd[2][i])\n\n"], "metadata": {}}, {"execution_count": null, "outputs": [], "cell_type": "code", "source": ["\n \n\n"], "metadata": {}}], "metadata": {}, "nbformat_minor": 1}